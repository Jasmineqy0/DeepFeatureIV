{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from viz_utils import load_dfiv_model, predict_dfiv_model, load_dfiv_runs\n",
    "import plotly.express as px\n",
    "from src.data.demand_design_parcs_revise import psi, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>var waitForPlotly = setInterval( function() {if( typeof(window.Plotly) !== \"undefined\" ){MathJax.Hub.Config({ SVG: { font: \"STIX-Web\" }, displayAlign: \"center\" });MathJax.Hub.Queue([\"setRenderer\", MathJax.Hub, \"SVG\"]);clearInterval(waitForPlotly);}}, 250 );</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display, HTML\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "# The polling here is to ensure that plotly.js has already been loaded before\n",
    "# setting display alignment in order to avoid a race condition.\n",
    "display(HTML(\n",
    "    '<script>'\n",
    "        'var waitForPlotly = setInterval( function() {'\n",
    "            'if( typeof(window.Plotly) !== \"undefined\" ){'\n",
    "                'MathJax.Hub.Config({ SVG: { font: \"STIX-Web\" }, displayAlign: \"center\" });'\n",
    "                'MathJax.Hub.Queue([\"setRenderer\", MathJax.Hub, \"SVG\"]);'\n",
    "                'clearInterval(waitForPlotly);'\n",
    "            '}}, 250 );'\n",
    "    '</script>'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity, project = \"jasmineqy0\", \"formal_3\"  \n",
    "dup = 40\n",
    "rho = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs:  40\n"
     ]
    }
   ],
   "source": [
    "group = 'dfiv_low_dim_parcs_revise'\n",
    "runs_df = load_dfiv_runs(entity, project, filters={'group' : group})\n",
    "runs_df = runs_df[runs_df['state'].apply(lambda x: x == 'finished')]\n",
    "runs_df = runs_df[runs_df['config'].apply(lambda x: x['data_configs']['data_size'] == 10000)]\n",
    "runs_df = runs_df[runs_df['config'].apply(lambda x: x['data_configs']['rho'] == rho)]\n",
    "print('Number of runs: ', len(runs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs with test loss:  40\n",
      "min test loss: 473.4062805175781\n",
      "\n",
      "mean test loss:  23899.876779937746\n",
      "std test loss:  44501.56865893741\n"
     ]
    }
   ],
   "source": [
    "test_loss_idx = runs_df['summary'].apply(lambda x: True if 'test loss' in x else False)\n",
    "test_loss_df = runs_df[test_loss_idx]\n",
    "print('Number of runs with test loss: ', len(test_loss_df))\n",
    "\n",
    "original_test_loss = test_loss_df['summary'].apply(lambda x: x['min_test_loss'])\n",
    "\n",
    "min_idx, min_test_loss = np.argmin(original_test_loss), np.min(original_test_loss)\n",
    "print(f\"min test loss: {min_test_loss}\\n\")\n",
    "\n",
    "original_mean_test_loss = np.mean(original_test_loss)\n",
    "print('mean test loss: ', original_mean_test_loss)\n",
    "\n",
    "original_std_test_loss = np.std(original_test_loss)\n",
    "print('std test loss: ', original_std_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best run name: rural-energy-1405, best run id: m5fng05e\n",
      "best run path: jasmineqy0/formal_3/m5fng05e\n"
     ]
    }
   ],
   "source": [
    "best_run = runs_df.iloc[min_idx]\n",
    "assert best_run['summary']['min_test_loss'] == min_test_loss, 'min test loss not equal to min test loss in test loss df'\n",
    "best_run_name, best_run_id = best_run['name'], best_run['id']\n",
    "print(f'best run name: {best_run_name}, best run id: {best_run_id}')\n",
    "\n",
    "run_path_original = '/'.join([entity, project, best_run_id])\n",
    "print(f'best run path: {run_path_original}')\n",
    "\n",
    "dfiv_original_model = load_dfiv_model(run_path_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_rho = test_loss_df['config'].apply(lambda x: x['data_configs']['rho'])\n",
    "# original_test_sigma = test_loss_df['config'].apply(lambda x: x['data_configs']['sigma'])\n",
    "original_test_sigma = len(original_test_rho) * [0]\n",
    "\n",
    "original_sigma_seq = np.unique(np.sort(original_test_sigma)).tolist()\n",
    "\n",
    "df_original = pd.DataFrame({'sigma': original_test_sigma, 'rho': original_test_rho, 'test loss': original_test_loss})\n",
    "df_original = df_original.groupby(['sigma', 'rho']).mean().reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigma 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs:  40\n"
     ]
    }
   ],
   "source": [
    "group = 'dfiv_low_dim_noise_price_sigma_1'\n",
    "runs_df = load_dfiv_runs(entity, project, filters={'group' : group})\n",
    "runs_df = runs_df[runs_df['state'].apply(lambda x: x == 'finished')]\n",
    "runs_df = runs_df[runs_df['config'].apply(lambda x: x['data_configs']['data_size'] == 10000)]\n",
    "runs_df = runs_df[runs_df['config'].apply(lambda x: x['data_configs']['parcs_config'] == 'demand_noise_price_sigma_0.5')]\n",
    "print('Number of runs: ', len(runs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min test loss: 282.2323303222656\n",
      "\n",
      "mean test loss:  42525.09740600586\n",
      "std test loss:  73965.91159502012\n"
     ]
    }
   ],
   "source": [
    "test_loss_idx = runs_df['summary'].apply(lambda x: True if 'test loss' in x else False)\n",
    "test_loss_df = runs_df[test_loss_idx]\n",
    "\n",
    "sigma_half_test_loss = test_loss_df['summary'].apply(lambda x: x['min_test_loss'])\n",
    "\n",
    "min_idx, min_test_loss = np.argmin(sigma_half_test_loss), np.min(sigma_half_test_loss)\n",
    "best_run = runs_df.iloc[min_idx]\n",
    "print(f\"min test loss: {best_run['summary']['min_test_loss']}\\n\")\n",
    "\n",
    "sigma_half_mean_test_loss = np.mean(sigma_half_test_loss)\n",
    "print('mean test loss: ', sigma_half_mean_test_loss)\n",
    "\n",
    "sigma_half_std_test_loss = np.std(sigma_half_test_loss)\n",
    "print('std test loss: ', sigma_half_std_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best run name: lilac-morning-1048, best run id: 47de1jro\n",
      "best run path: jasmineqy0/formal_3/47de1jro\n"
     ]
    }
   ],
   "source": [
    "assert best_run['summary']['min_test_loss'] == min_test_loss, 'min test loss not equal to min test loss in test loss df'\n",
    "best_run_name, best_run_id = best_run['name'], best_run['id']\n",
    "print(f'best run name: {best_run_name}, best run id: {best_run_id}')\n",
    "\n",
    "run_path_sigma_half = '/'.join([entity, project, best_run_id])\n",
    "print(f'best run path: {run_path_sigma_half}')\n",
    "\n",
    "dfiv_hetero_sigma_half_model = load_dfiv_model(run_path_sigma_half)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigma 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs:  40\n"
     ]
    }
   ],
   "source": [
    "group = 'dfiv_low_dim_noise_price_sigma_1'\n",
    "runs_df = load_dfiv_runs(entity, project, filters={'group' : group})\n",
    "runs_df = runs_df[runs_df['state'].apply(lambda x: x == 'finished')]\n",
    "runs_df = runs_df[runs_df['config'].apply(lambda x: x['data_configs']['data_size'] == 10000)]\n",
    "runs_df = runs_df[runs_df['config'].apply(lambda x: x['data_configs']['parcs_config'] == 'demand_noise_price_sigma_2')]\n",
    "print('Number of runs: ', len(runs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min test loss: 282.2323303222656\n",
      "\n",
      "mean test loss:  20794.825450134278\n",
      "std test loss:  43628.72628495271\n"
     ]
    }
   ],
   "source": [
    "test_loss_idx = runs_df['summary'].apply(lambda x: True if 'min_test_loss' in x else False)\n",
    "test_loss_df = runs_df[test_loss_idx]\n",
    "\n",
    "sigma_2_test_loss = test_loss_df['summary'].apply(lambda x: x['min_test_loss'])\n",
    "\n",
    "min_idx, min_test_loss = np.argmin(sigma_2_test_loss), np.min(sigma_2_test_loss)\n",
    "print(f\"min test loss: {best_run['summary']['min_test_loss']}\\n\")\n",
    "\n",
    "sigma_2_mean_test_loss = np.mean(sigma_2_test_loss)\n",
    "print('mean test loss: ', sigma_2_mean_test_loss)\n",
    "\n",
    "sigma_2_std_test_loss = np.std(sigma_2_test_loss)\n",
    "print('std test loss: ', sigma_2_std_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best run name: celestial-cosmos-1104, best run id: mn0g210k\n",
      "best run path: jasmineqy0/formal_3/mn0g210k\n"
     ]
    }
   ],
   "source": [
    "best_run = runs_df.iloc[min_idx]\n",
    "assert best_run['summary']['min_test_loss'] == min_test_loss, 'min test loss not equal to min test loss in test loss df'\n",
    "best_run_name, best_run_id = best_run['name'], best_run['id']\n",
    "print(f'best run name: {best_run_name}, best run id: {best_run_id}')\n",
    "\n",
    "run_path_sigma_2 = '/'.join([entity, project, best_run_id])\n",
    "print(f'best run path: {run_path_sigma_2}')\n",
    "\n",
    "dfiv_hetero_sigma_2_model = load_dfiv_model(run_path_sigma_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigma 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs:  40\n"
     ]
    }
   ],
   "source": [
    "group = 'dfiv_low_dim_noise_price_sigma_1'\n",
    "runs_df = load_dfiv_runs(entity, project, filters={'group' : group})\n",
    "runs_df = runs_df[runs_df['state'].apply(lambda x: x == 'finished')]\n",
    "runs_df = runs_df[runs_df['config'].apply(lambda x: x['data_configs']['data_size'] == 10000)]\n",
    "runs_df = runs_df[runs_df['config'].apply(lambda x: x['data_configs']['parcs_config'] == 'demand_noise_price_sigma_4')]\n",
    "print('Number of runs: ', len(runs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min test loss: 867.0722045898438\n",
      "\n",
      "mean test loss:  19411.78975830078\n",
      "std test loss:  42774.05515712865\n"
     ]
    }
   ],
   "source": [
    "test_loss_idx = runs_df['summary'].apply(lambda x: True if 'test loss' in x else False)\n",
    "test_loss_df = runs_df[test_loss_idx]\n",
    "\n",
    "sigma_4_test_loss = test_loss_df['summary'].apply(lambda x: x['min_test_loss'])\n",
    "\n",
    "min_idx, min_test_loss = np.argmin(sigma_4_test_loss), np.min(sigma_4_test_loss)\n",
    "print(f\"min test loss: {best_run['summary']['min_test_loss']}\\n\")\n",
    "\n",
    "sigma_4_mean_test_loss = np.mean(sigma_4_test_loss)\n",
    "print('mean test loss: ', sigma_4_mean_test_loss)\n",
    "\n",
    "sigma_4_std_test_loss = np.std(sigma_4_test_loss)\n",
    "print('std test loss: ', sigma_4_std_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best run name: generous-music-1257, best run id: m3xb61gu\n",
      "best run path: jasmineqy0/formal_3/m3xb61gu\n"
     ]
    }
   ],
   "source": [
    "best_run = runs_df.iloc[min_idx]\n",
    "assert best_run['summary']['min_test_loss'] == min_test_loss, 'min test loss not equal to min test loss in test loss df'\n",
    "best_run_name, best_run_id = best_run['name'], best_run['id']\n",
    "print(f'best run name: {best_run_name}, best run id: {best_run_id}')\n",
    "\n",
    "run_path_sigma_4 = '/'.join([entity, project, best_run_id])\n",
    "print(f'best run path: {run_path_sigma_4}')\n",
    "\n",
    "dfiv_hetero_sigma_4_model = load_dfiv_model(run_path_sigma_4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigma 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs:  40\n"
     ]
    }
   ],
   "source": [
    "group = 'dfiv_low_dim_noise_price_sigma_1'\n",
    "runs_df = load_dfiv_runs(entity, project, filters={'group' : group})\n",
    "runs_df = runs_df[runs_df['state'].apply(lambda x: x == 'finished')]\n",
    "runs_df = runs_df[runs_df['config'].apply(lambda x: x['data_configs']['data_size'] == 10000)]\n",
    "runs_df = runs_df[runs_df['config'].apply(lambda x: x['data_configs']['parcs_config'] == 'demand_noise_price_sigma_8')]\n",
    "print('Number of runs: ', len(runs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min test loss: 1749.1513671875\n",
      "\n",
      "mean test loss:  14172.035009765625\n",
      "std test loss:  9891.87679881794\n"
     ]
    }
   ],
   "source": [
    "test_loss_idx = runs_df['summary'].apply(lambda x: True if 'test loss' in x else False)\n",
    "test_loss_df = runs_df[test_loss_idx]\n",
    "\n",
    "sigma_8_test_loss = test_loss_df['summary'].apply(lambda x: x['min_test_loss'])\n",
    "\n",
    "min_idx, min_test_loss = np.argmin(sigma_8_test_loss), np.min(sigma_8_test_loss)\n",
    "print(f\"min test loss: {best_run['summary']['min_test_loss']}\\n\")\n",
    "\n",
    "sigma_8_mean_test_loss = np.mean(sigma_8_test_loss)\n",
    "print('mean test loss: ', sigma_8_mean_test_loss)\n",
    "\n",
    "sigma_8_std_test_loss = np.std(sigma_8_test_loss)\n",
    "print('std test loss: ', sigma_8_std_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best run name: sweet-flower-1360, best run id: de9gofga\n",
      "best run path: jasmineqy0/formal_3/de9gofga\n"
     ]
    }
   ],
   "source": [
    "best_run = runs_df.iloc[min_idx]\n",
    "assert best_run['summary']['min_test_loss'] == min_test_loss, 'min test loss not equal to min test loss in test loss df'\n",
    "best_run_name, best_run_id = best_run['name'], best_run['id']\n",
    "print(f'best run name: {best_run_name}, best run id: {best_run_id}')\n",
    "\n",
    "run_path_sigma_8 = '/'.join([entity, project, best_run_id])\n",
    "print(f'best run path: {run_path_sigma_8}')\n",
    "\n",
    "dfiv_hetero_sigma_8_model = load_dfiv_model(run_path_sigma_8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_point = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vary t: fix p = 25, S = E[S]\n",
    "emotion_range = np.array([1, 2, 3, 4, 5, 6, 7])\n",
    "price_val = 25\n",
    "price = np.ones(num_point) * price_val\n",
    "time = np.linspace(0.0, 10, num_point)\n",
    "emotion = (np.ones(num_point) * np.mean(emotion_range))\n",
    "covariate = np.c_[time, emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfiv_hetero_mu_4_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m price \u001b[39m=\u001b[39m price[:, np\u001b[39m.\u001b[39mnewaxis]\n\u001b[1;32m      4\u001b[0m original_demand \u001b[39m=\u001b[39m predict_dfiv_model(dfiv_original_model, price, covariate)\n\u001b[0;32m----> 5\u001b[0m mu_4_pred_demand \u001b[39m=\u001b[39m predict_dfiv_model(dfiv_hetero_mu_4_model, price, covariate)\n\u001b[1;32m      6\u001b[0m mu_8_pred_demand \u001b[39m=\u001b[39m predict_dfiv_model(dfiv_hetero_mu_8_model, price, covariate)\n\u001b[1;32m      7\u001b[0m mu_16_pred_demand \u001b[39m=\u001b[39m predict_dfiv_model(dfiv_hetero_mu_16_model, price, covariate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfiv_hetero_mu_4_model' is not defined"
     ]
    }
   ],
   "source": [
    "true_demand = f(price, time, emotion)\n",
    "\n",
    "price = price[:, np.newaxis]\n",
    "original_demand = predict_dfiv_model(dfiv_original_model, price, covariate)\n",
    "mu_4_pred_demand = predict_dfiv_model(dfiv_hetero_mu_4_model, price, covariate)\n",
    "mu_8_pred_demand = predict_dfiv_model(dfiv_hetero_mu_8_model, price, covariate)\n",
    "mu_16_pred_demand = predict_dfiv_model(dfiv_hetero_mu_16_model, price, covariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "time = time.squeeze()\n",
    "l1 = ax.plot(time, true_demand, color='red', label='Truth')\n",
    "l2 = ax.plot(time, original_demand, color='blue', label='DFIV: original')\n",
    "l3 = ax.plot(time, mu_4_pred_demand, color='pink', label='DFIV: $\\mu(V)=4$ ')\n",
    "l4 = ax.plot(time, mu_8_pred_demand, color='orange', label='DFIV: $\\mu(V)=8$ ')\n",
    "l5 = ax.plot(time, mu_16_pred_demand, color='green', label='DFIV: $\\mu(V)=16$ ')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Time of the year t')\n",
    "ax.set_ylabel('$\\mathbb{E}[Y\\mid do(P=25), T=t]$')\n",
    "plt.savefig(f'images/CATE_mean_mu.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_point = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vary p: fix t = E[t] = 5, S = E[S] = 4\n",
    "emotion_range = np.array([1, 2, 3, 4, 5, 6, 7])\n",
    "price = np.ones(num_point) * np.linspace(10, 25, num_point)\n",
    "time = np.linspace(0.0, 10, num_point)\n",
    "emotion = (np.ones(num_point) * np.mean(emotion_range))\n",
    "covariate = np.c_[time, emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.mean(psi(time)) * np.mean(emotion_range)\n",
    "coef = G - 2\n",
    "offset = 100 + 10 * G\n",
    "\n",
    "true_demand = coef * price + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vary p: fix t = E[t] = 5, S = E[S] = 4\n",
    "# emotion_range = np.array([1, 2, 3, 4, 5, 6, 7])\n",
    "# price_val = np.linspace(10, 25, num_point)\n",
    "# price = np.ones(num_point) * price_val\n",
    "# time = np.array([5] * num_point)\n",
    "# emotion = (np.ones(num_point) * np.mean(emotion_range))\n",
    "# covariate = np.c_[time, emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_demand = []\n",
    "for p in price:\n",
    "    p = (np.ones(num_point) * p)[:, np.newaxis]\n",
    "    pred = np.mean((predict_dfiv_model(dfiv_original_model, p, covariate)))\n",
    "    original_demand.append(pred)\n",
    "\n",
    "mu_4_pred_demand = []\n",
    "for p in price:\n",
    "    p = (np.ones(num_point) * p)[:, np.newaxis]\n",
    "    pred = np.mean((predict_dfiv_model(dfiv_hetero_mu_4_model, p, covariate)))\n",
    "    mu_4_pred_demand.append(pred)\n",
    "    \n",
    "mu_8_pred_demand = []\n",
    "for p in price:\n",
    "    p = (np.ones(num_point) * p)[:, np.newaxis]\n",
    "    pred = np.mean((predict_dfiv_model(dfiv_hetero_mu_8_model, p, covariate)))\n",
    "    mu_8_pred_demand.append(pred)\n",
    "    \n",
    "mu_16_pred_demand = []\n",
    "for p in price:\n",
    "    p = (np.ones(num_point) * p)[:, np.newaxis]\n",
    "    pred = np.mean((predict_dfiv_model(dfiv_hetero_mu_16_model, p, covariate)))\n",
    "    mu_16_pred_demand.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "price = price.squeeze()\n",
    "l1 = ax.plot(time, true_demand, color='red', label='Truth')\n",
    "l2 = ax.plot(time, original_demand, color='blue', label='DFIV: original')\n",
    "l3 = ax.plot(time, mu_4_pred_demand, color='pink', label='DFIV: $\\mu(V)=4$ ')\n",
    "l4 = ax.plot(time, mu_8_pred_demand, color='orange', label='DFIV: $\\mu(V)=8$ ')\n",
    "l5 = ax.plot(time, mu_16_pred_demand, color='green', label='DFIV: $\\mu(V)=16$ ')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Time t')\n",
    "ax.set_ylabel('$\\mathbb{E}[Y\\mid do(P=p)]$')\n",
    "plt.savefig(f'images/ATE_mean_mu.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average DFIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = [0, 4, 8, 16]\n",
    "test_mean_loss = [original_mean_test_loss, mu_4_mean_test_loss, mu_8_mean_test_loss, mu_16_mean_test_loss]\n",
    "test_loss = [original_test_loss, mu_4_test_loss, mu_8_test_loss, mu_16_test_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_loss = pd.DataFrame({'mu': mu, 'test_loss': test_mean_loss})\n",
    "fig = px.bar(df_mean_loss, x='mu', y='test_loss',title='Test loss for different mean V',\n",
    "             category_orders={'mu': mu})\n",
    "fig.update_xaxes(type='category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "mu = [[i] * dup for i in mu]\n",
    "mu = list(chain(*mu))\n",
    "test_loss = np.hstack(test_loss)\n",
    "\n",
    "df_loss = pd.DataFrame({'mu': mu, 'test_loss': test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df_loss, x='mu', y='test_loss',title='Test loss for different V', log_y=True)\n",
    "fig.update_xaxes(type='category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
